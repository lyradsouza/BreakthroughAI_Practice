{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "python_kernel",
      "language": "python",
      "name": "python_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "module8(Lyra).notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyradsouza/BreakthroughAI_Practice/blob/main/module8(Lyra)_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63118e0e"
      },
      "source": [
        "# Lab 8 - Deep Learning 3"
      ],
      "id": "63118e0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6deb3d5"
      },
      "source": [
        "The goal of this week's lab is to learn to use another widely-used neural network module: recurrent neural networks (RNNs). We can use it to learn features from sequences such as time series and text."
      ],
      "id": "b6deb3d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dbf2af7"
      },
      "source": [
        "![image](https://www.researchgate.net/profile/Huy-Tien-Nguyen/publication/321259272/figure/fig2/AS:572716866433034@1513557749934/Illustration-of-our-LSTM-model-for-sentiment-classification-Each-word-is-transfered-to-a_W640.jpg)"
      ],
      "id": "0dbf2af7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de11e2a9"
      },
      "source": [
        "How should we extract features from sequences, which might be of variable length? Recurrent Neural Networks (RNNs) provide a solution by summarizing the entire sequence into a fixed-size vector representation. This week we will walk through how to use RNNs for sequence processing."
      ],
      "id": "de11e2a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3636875d"
      },
      "source": [
        "* **Review**: Convolutional Neural Networks (CNNs)\n",
        "* **Unit A**: Time Series Classification and Recurrent Neural Networks (RNNs)\n",
        "* **Unit B**: Recurrent Neural Networks for Text Classification"
      ],
      "id": "3636875d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "814ad1ce"
      },
      "source": [
        "## Review"
      ],
      "id": "814ad1ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15a9a0fe"
      },
      "source": [
        "Last time we learned the basics of convolutional neural networks (CNNs) and used them for image classification."
      ],
      "id": "15a9a0fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:05:57.924793Z",
          "iopub.status.busy": "2021-08-05T17:05:57.923890Z",
          "iopub.status.idle": "2021-08-05T17:06:05.571919Z",
          "shell.execute_reply": "2021-08-05T17:06:05.572969Z"
        },
        "id": "0e29fe7f"
      },
      "source": [
        "# For Tables\n",
        "import pandas as pd\n",
        "# For Visualization\n",
        "import altair as alt\n",
        "# For Scikit-Learn\n",
        "import sklearn\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# For Neural Networks\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Reshape"
      ],
      "id": "0e29fe7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd8dd03"
      },
      "source": [
        "We will also turn off warnings."
      ],
      "id": "3dd8dd03"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:05.579148Z",
          "iopub.status.busy": "2021-08-05T17:06:05.578323Z",
          "iopub.status.idle": "2021-08-05T17:06:05.580599Z",
          "shell.execute_reply": "2021-08-05T17:06:05.581066Z"
        },
        "id": "375ed265"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "375ed265",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f03c95b6"
      },
      "source": [
        "We saw in last week how to store images and their labels in Pandas dataframes."
      ],
      "id": "f03c95b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:05.587309Z",
          "iopub.status.busy": "2021-08-05T17:06:05.586183Z",
          "iopub.status.idle": "2021-08-05T17:06:23.270341Z",
          "shell.execute_reply": "2021-08-05T17:06:23.271646Z"
        },
        "id": "53a619e2"
      },
      "source": [
        "df_train = pd.read_csv('https://srush.github.io/BT-AI/notebooks/mnist_train.csv.gz', compression='gzip')\n",
        "df_test = pd.read_csv('https://srush.github.io/BT-AI/notebooks/mnist_test.csv.gz', compression='gzip')"
      ],
      "id": "53a619e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21c9aa96"
      },
      "source": [
        "The column `class` stores the class of each image, which is a number between 0 and 9."
      ],
      "id": "21c9aa96"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:23.303053Z",
          "iopub.status.busy": "2021-08-05T17:06:23.301474Z",
          "iopub.status.idle": "2021-08-05T17:06:23.367500Z",
          "shell.execute_reply": "2021-08-05T17:06:23.368078Z"
        },
        "id": "2d90cbe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6e60c4-c202-49df-b477-4224e8b96775"
      },
      "source": [
        "df_train[:100][\"class\"].unique()"
      ],
      "id": "2d90cbe9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 3, 6, 7, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33e444a"
      },
      "source": [
        "The rest of columns store the features, where we have 784 features since our images are 28x28. Each feature stores the intensity at each pixel : for instance, the column \"3x4\" stores the pixel value at the 3rd row and the 4th column. Since the size of each image is 28x28, there are 28 rows and 28 columns."
      ],
      "id": "d33e444a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "369e49f2"
      },
      "source": [
        "To make later processing easier, we store the names of pixel value columns in a list `features`."
      ],
      "id": "369e49f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:23.377237Z",
          "iopub.status.busy": "2021-08-05T17:06:23.376283Z",
          "iopub.status.idle": "2021-08-05T17:06:23.379635Z",
          "shell.execute_reply": "2021-08-05T17:06:23.380165Z"
        },
        "id": "0ca90f8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5e9361-f600-401a-9901-91bc627795cf"
      },
      "source": [
        "features = []\n",
        "for i in range(1, 29):\n",
        "    for j in range(1, 29):\n",
        "        features.append(str(i) + \"x\" + str(j))\n",
        "len(features)"
      ],
      "id": "0ca90f8a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2c5bb92"
      },
      "source": [
        "We used the below utility functions for visualizing the images."
      ],
      "id": "d2c5bb92"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9e07eff"
      },
      "source": [
        "Convert feature to x, y, and value."
      ],
      "id": "f9e07eff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:23.393744Z",
          "iopub.status.busy": "2021-08-05T17:06:23.391118Z",
          "iopub.status.idle": "2021-08-05T17:06:23.394684Z",
          "shell.execute_reply": "2021-08-05T17:06:23.395221Z"
        },
        "lines_to_next_cell": 1,
        "id": "21e2fe5e"
      },
      "source": [
        "def position(row):\n",
        "    y, x = row[\"index\"].split(\"x\")\n",
        "    return {\"x\":int(x),\n",
        "            \"y\":int(y),\n",
        "            \"val\":row[\"val\"]}\n",
        "def draw_image(i, shuffle=False):\n",
        "    t = df_train[i:i+1].T.reset_index().rename(columns={i: \"val\"})\n",
        "    out = t.loc[t[\"index\"] != \"class\"].apply(position, axis=1, result_type=\"expand\")\n",
        "\n",
        "    label = df_train.loc[i][\"class\"]\n",
        "    title = \"Image of a \" + str(label)\n",
        "    if shuffle:\n",
        "        out[\"val\"] = sklearn.utils.shuffle(out[\"val\"], random_state=1234).reset_index()[\"val\"]\n",
        "        title = \"Shuffled Image of a \" + str(label)\n",
        "        \n",
        "    return (alt.Chart(out)\n",
        "            .mark_rect()\n",
        "            .properties(title=title)\n",
        "            .encode(\n",
        "                x=\"x:O\",\n",
        "                y=\"y:O\",\n",
        "                fill=\"val:Q\",\n",
        "                color=\"val:Q\",\n",
        "                tooltip=(\"x\", \"y\", \"val\")\n",
        "            ))"
      ],
      "id": "21e2fe5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f107b71"
      },
      "source": [
        "We can visualize an example image."
      ],
      "id": "1f107b71"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:23.401820Z",
          "iopub.status.busy": "2021-08-05T17:06:23.400735Z",
          "iopub.status.idle": "2021-08-05T17:06:24.215624Z",
          "shell.execute_reply": "2021-08-05T17:06:24.216205Z"
        },
        "lines_to_next_cell": 1,
        "id": "4095179a"
      },
      "source": [
        "im = draw_image(0)\n",
        "im"
      ],
      "id": "4095179a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92646a31"
      },
      "source": [
        "The task is to classify the label given an image. To do that, we first need to define a function that creates our model."
      ],
      "id": "92646a31"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f1e837a"
      },
      "source": [
        "Here is what a CNN model looks like. It contains two convolution layers and two max pooling layers."
      ],
      "id": "6f1e837a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:24.227548Z",
          "iopub.status.busy": "2021-08-05T17:06:24.225973Z",
          "iopub.status.idle": "2021-08-05T17:06:24.228879Z",
          "shell.execute_reply": "2021-08-05T17:06:24.229487Z"
        },
        "lines_to_next_cell": 1,
        "id": "c58dac31"
      },
      "source": [
        "def create_cnn_model():\n",
        "    # create model\n",
        "    input_shape = (28, 28, 1)\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape))\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation=\"softmax\")) # output a vector of size 10\n",
        "    # Compile model\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                   optimizer=\"adam\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "c58dac31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d127cd8"
      },
      "source": [
        "Then we create the model and fit it on training data."
      ],
      "id": "4d127cd8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:06:24.236689Z",
          "iopub.status.busy": "2021-08-05T17:06:24.235929Z",
          "iopub.status.idle": "2021-08-05T17:08:16.821409Z",
          "shell.execute_reply": "2021-08-05T17:08:16.823302Z"
        },
        "id": "8073e929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0c02e0-dfbb-47d6-8ba9-fd30d3c6720a"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_cnn_model,\n",
        "                         epochs=2,\n",
        "                         batch_size=20,\n",
        "                         verbose=1)\n",
        "# fit model\n",
        "model.fit(x=df_train[features].astype(float),\n",
        "          y=df_train[\"class\"])"
      ],
      "id": "8073e929",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "3000/3000 [==============================] - 60s 19ms/step - loss: 0.9874 - accuracy: 0.8976\n",
            "Epoch 2/2\n",
            "3000/3000 [==============================] - 59s 20ms/step - loss: 0.0744 - accuracy: 0.9787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f670f0ddc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1436e9d"
      },
      "source": [
        "With a trained model, we can apply it to the test dataset and measure the test accuracy."
      ],
      "id": "e1436e9d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:08:16.843846Z",
          "iopub.status.busy": "2021-08-05T17:08:16.839986Z",
          "iopub.status.idle": "2021-08-05T17:08:19.304624Z",
          "shell.execute_reply": "2021-08-05T17:08:19.305504Z"
        },
        "id": "ed239000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60940a1-fce4-4432-fb98-6978daae1c3f"
      },
      "source": [
        "df_test[\"predict\"] = model.predict(df_test[features])\n",
        "correct = (df_test[\"predict\"] == df_test[\"class\"])\n",
        "accuracy = correct.sum() / correct.size\n",
        "print (\"accuracy: \", accuracy)"
      ],
      "id": "ed239000",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 4s 7ms/step\n",
            "accuracy:  0.973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da05f0d9"
      },
      "source": [
        "### Review Exercise"
      ],
      "id": "da05f0d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10215181"
      },
      "source": [
        "Change the model above to have the kernel size of convolution layers to be (1, 1). How does this affect the performance? Why?"
      ],
      "id": "10215181"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:08:20.416948Z",
          "iopub.status.busy": "2021-08-05T17:08:19.317564Z",
          "iopub.status.idle": "2021-08-05T17:08:56.198841Z",
          "shell.execute_reply": "2021-08-05T17:08:56.199447Z"
        },
        "id": "1392dd02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb107e83-8973-4be3-b679-2d9784ffc8b7"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "def create_cnn_model():\n",
        "    # create model\n",
        "    input_shape = (28, 28, 1)\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape))\n",
        "    model.add(Conv2D(32, kernel_size=(1, 1), activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(64, kernel_size=(1, 1), activation=\"relu\"))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation=\"softmax\")) # output a vector of size 10\n",
        "    # Compile model\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                   optimizer=\"adam\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "    return model\n",
        "model = KerasClassifier(build_fn=create_cnn_model,\n",
        "                         epochs=2,\n",
        "                         batch_size=20,\n",
        "                         verbose=1)\n",
        "# fit model\n",
        "model.fit(x=df_train[features].astype(float),\n",
        "          y=df_train[\"class\"])\n",
        "df_test[\"predict\"] = model.predict(df_test[features])\n",
        "correct = (df_test[\"predict\"] == df_test[\"class\"])\n",
        "accuracy = correct.sum() / correct.size\n",
        "print (\"accuracy: \", accuracy)"
      ],
      "id": "1392dd02",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "3000/3000 [==============================] - 40s 13ms/step - loss: 1.5915 - accuracy: 0.7495\n",
            "Epoch 2/2\n",
            "3000/3000 [==============================] - 38s 13ms/step - loss: 0.6027 - accuracy: 0.8062\n",
            "500/500 [==============================] - 2s 4ms/step\n",
            "accuracy:  0.8085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD8j2lJHmd6v"
      },
      "source": [
        "# The performance is worse. A too small kernel results in too individual of a pattern, since it's just each pixel."
      ],
      "id": "CD8j2lJHmd6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79274762"
      },
      "source": [
        "Change the model above to have the kernel size of the first convolution layer to be (28, 28), and remove other convolution and pooling layers. How does this affect the performance? Why?"
      ],
      "id": "79274762"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:08:56.458063Z",
          "iopub.status.busy": "2021-08-05T17:08:56.457204Z",
          "iopub.status.idle": "2021-08-05T17:09:05.572491Z",
          "shell.execute_reply": "2021-08-05T17:09:05.573017Z"
        },
        "id": "6889ef40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675d88c3-b27e-420c-a63b-b1c1b658922c"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "def create_cnn_model():\n",
        "    # create model\n",
        "    input_shape = (28, 28, 1)\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape))\n",
        "    model.add(Conv2D(32, kernel_size=(28, 28), activation=\"relu\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation=\"softmax\")) # output a vector of size 10\n",
        "    # Compile model\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                   optimizer=\"adam\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "    return model\n",
        "model = KerasClassifier(build_fn=create_cnn_model,\n",
        "                         epochs=2,\n",
        "                         batch_size=20,\n",
        "                         verbose=1)\n",
        "# fit model\n",
        "model.fit(x=df_train[features].astype(float),\n",
        "          y=df_train[\"class\"])\n",
        "df_test[\"predict\"] = model.predict(df_test[features])\n",
        "correct = (df_test[\"predict\"] == df_test[\"class\"])\n",
        "accuracy = correct.sum() / correct.size\n",
        "print (\"accuracy: \", accuracy)"
      ],
      "id": "6889ef40",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "3000/3000 [==============================] - 7s 2ms/step - loss: 1.6135 - accuracy: 0.5780\n",
            "Epoch 2/2\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.5465 - accuracy: 0.8574\n",
            "500/500 [==============================] - 1s 1ms/step\n",
            "accuracy:  0.8903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuw-H75PmP6M"
      },
      "source": [
        "# The performance is once again worse than the original. The kernel size is just the size of the image,\n",
        "# so the model is not able to match the pattern since it's so big\n",
        "\n",
        "\n",
        "# A good kernel size is either 3,3 or 5,5"
      ],
      "id": "uuw-H75PmP6M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e8ec9c9"
      },
      "source": [
        "## Unit A"
      ],
      "id": "7e8ec9c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db980117"
      },
      "source": [
        "### Time Series Classification and Recurrent Neural Networks (RNNs)"
      ],
      "id": "db980117"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6f6fd95"
      },
      "source": [
        "Just like CNNs are suitable for the processing of 2-D images (and 1-D sequences since they can be viewed as a special case of 2-D images), recurrent neural networks (RNNs) are suitable for 1-D sequence modeling."
      ],
      "id": "a6f6fd95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c0432e4"
      },
      "source": [
        "Let's start from a concrete sequence classification example, where we want to classify a curve into one of three possible classes: rectangle, triangle, and ellipse:"
      ],
      "id": "6c0432e4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:05.585179Z",
          "iopub.status.busy": "2021-08-05T17:09:05.584249Z",
          "iopub.status.idle": "2021-08-05T17:09:05.699388Z",
          "shell.execute_reply": "2021-08-05T17:09:05.699855Z"
        },
        "id": "135b25b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "1b7cdeb7-68c4-40c1-cee6-d1ef4d82c188"
      },
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/srush/BT-AI/main/notebooks/shape_classification_train.csv')\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/srush/BT-AI/main/notebooks/shape_classification_test.csv')\n",
        "df_train"
      ],
      "id": "135b25b6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>square</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>square</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>triangle</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>1.040000</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>triangle</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>square</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>ellipse</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.359011</td>\n",
              "      <td>0.498888</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.679869</td>\n",
              "      <td>0.745356</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.845905</td>\n",
              "      <td>0.884433</td>\n",
              "      <td>0.916515</td>\n",
              "      <td>0.942809</td>\n",
              "      <td>0.963789</td>\n",
              "      <td>0.979796</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.997775</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997775</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.979796</td>\n",
              "      <td>0.963789</td>\n",
              "      <td>0.942809</td>\n",
              "      <td>0.916515</td>\n",
              "      <td>0.884433</td>\n",
              "      <td>0.845905</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.745356</td>\n",
              "      <td>0.679869</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.498888</td>\n",
              "      <td>0.359011</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>ellipse</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.320145</td>\n",
              "      <td>0.446594</td>\n",
              "      <td>0.539313</td>\n",
              "      <td>0.613784</td>\n",
              "      <td>0.676065</td>\n",
              "      <td>0.729285</td>\n",
              "      <td>0.775312</td>\n",
              "      <td>0.815365</td>\n",
              "      <td>0.850289</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.907036</td>\n",
              "      <td>0.929659</td>\n",
              "      <td>0.948829</td>\n",
              "      <td>0.964753</td>\n",
              "      <td>0.977588</td>\n",
              "      <td>0.987456</td>\n",
              "      <td>0.994444</td>\n",
              "      <td>0.998614</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998614</td>\n",
              "      <td>0.994444</td>\n",
              "      <td>0.987456</td>\n",
              "      <td>0.977588</td>\n",
              "      <td>0.964753</td>\n",
              "      <td>0.948829</td>\n",
              "      <td>0.929659</td>\n",
              "      <td>0.907036</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.850289</td>\n",
              "      <td>0.815365</td>\n",
              "      <td>0.775312</td>\n",
              "      <td>0.729285</td>\n",
              "      <td>0.676065</td>\n",
              "      <td>0.613784</td>\n",
              "      <td>0.539313</td>\n",
              "      <td>0.446594</td>\n",
              "      <td>0.320145</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>square</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>ellipse</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.515079</td>\n",
              "      <td>0.699854</td>\n",
              "      <td>0.820652</td>\n",
              "      <td>0.903508</td>\n",
              "      <td>0.958315</td>\n",
              "      <td>0.989743</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.989743</td>\n",
              "      <td>0.958315</td>\n",
              "      <td>0.903508</td>\n",
              "      <td>0.820652</td>\n",
              "      <td>0.699854</td>\n",
              "      <td>0.515079</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>triangle</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows √ó 56 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         class  0  1  2  3  4    5    6  ...        47   48  49  50  51  52  53  54\n",
              "0       square  0  0  0  0  0  0.0  0.0  ...  0.000000  0.0   0   0   0   0   0   0\n",
              "1       square  0  0  0  0  0  0.0  0.0  ...  1.000000  0.0   0   0   0   0   0   0\n",
              "2     triangle  0  0  0  0  0  0.0  0.0  ...  0.000000  0.0   0   0   0   0   0   0\n",
              "3     triangle  0  0  0  0  0  0.0  0.0  ...  0.000000  0.0   0   0   0   0   0   0\n",
              "4       square  0  0  0  0  0  0.0  0.0  ...  0.000000  0.0   0   0   0   0   0   0\n",
              "...        ... .. .. .. .. ..  ...  ...  ...       ...  ...  ..  ..  ..  ..  ..  ..\n",
              "1195   ellipse  0  0  0  0  0  0.0  0.0  ...  0.000000  0.0   0   0   0   0   0   0\n",
              "1196   ellipse  0  0  0  0  0  0.0  0.0  ...  0.320145  0.0   0   0   0   0   0   0\n",
              "1197    square  0  0  0  0  0  0.0  0.0  ...  0.000000  0.0   0   0   0   0   0   0\n",
              "1198   ellipse  0  0  0  0  0  0.0  0.0  ...  0.000000  0.0   0   0   0   0   0   0\n",
              "1199  triangle  0  0  0  0  0  0.0  0.0  ...  0.071429  0.0   0   0   0   0   0   0\n",
              "\n",
              "[1200 rows x 56 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ecb073"
      },
      "source": [
        "In this example, the curve is stored as a vector with 55 entries. In the dataframe, the i-th entry of this vector is stored in a column named $i$.\n",
        "As before, we store the names of feature columns in a list `features`."
      ],
      "id": "65ecb073"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:05.704417Z",
          "iopub.status.busy": "2021-08-05T17:09:05.703743Z",
          "iopub.status.idle": "2021-08-05T17:09:05.705900Z",
          "shell.execute_reply": "2021-08-05T17:09:05.706364Z"
        },
        "id": "25af596f"
      },
      "source": [
        "input_length = 55\n",
        "features = []\n",
        "for i in range(input_length):\n",
        "    features.append(str(i))"
      ],
      "id": "25af596f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faafcc8c"
      },
      "source": [
        "We can visualize some examples using the following function."
      ],
      "id": "faafcc8c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:05.714601Z",
          "iopub.status.busy": "2021-08-05T17:09:05.713532Z",
          "iopub.status.idle": "2021-08-05T17:09:06.888048Z",
          "shell.execute_reply": "2021-08-05T17:09:06.888610Z"
        },
        "lines_to_next_cell": 1,
        "id": "c533c279"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def draw_curve(i):\n",
        "    t = df_train[features].iloc[i]\n",
        "    c = df_train['class'].iloc[i]\n",
        "    plt.plot(list(t))\n",
        "    plt.title(f'class {c}')\n",
        "    plt.show()\n",
        "draw_curve(0)\n",
        "draw_curve(3)\n",
        "draw_curve(5)"
      ],
      "id": "c533c279",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd325f8e"
      },
      "source": [
        "First, let's try to apply an MLP classifier to this problem. Note that we need to set the size of the last layer to be 3 since there are three output classes."
      ],
      "id": "fd325f8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:06.895019Z",
          "iopub.status.busy": "2021-08-05T17:09:06.894334Z",
          "iopub.status.idle": "2021-08-05T17:09:06.896701Z",
          "shell.execute_reply": "2021-08-05T17:09:06.897173Z"
        },
        "lines_to_next_cell": 1,
        "id": "d2255005"
      },
      "source": [
        "def create_mlp_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax')) # output a vector of size 3\n",
        "    # Compile model\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "id": "d2255005",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:06.913153Z",
          "iopub.status.busy": "2021-08-05T17:09:06.912128Z",
          "iopub.status.idle": "2021-08-05T17:09:08.379387Z",
          "shell.execute_reply": "2021-08-05T17:09:08.379961Z"
        },
        "lines_to_next_cell": 2,
        "id": "f429bcc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b15cf4-3357-4035-be5b-e99db07cef65"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_mlp_model,\n",
        "                        epochs=10,\n",
        "                        batch_size=20,\n",
        "                        verbose=1)\n",
        "# fit model\n",
        "model.fit(x=df_train[features], y=df_train[\"class\"])\n",
        "# print summary\n",
        "print (model.model.summary())\n",
        "# predict on test set\n",
        "df_test[\"predict\"] = model.predict(df_test[features])\n",
        "correct = (df_test[\"predict\"] == df_test[\"class\"])\n",
        "accuracy = correct.sum() / correct.size\n",
        "print (\"accuracy: \", accuracy)"
      ],
      "id": "f429bcc1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60/60 [==============================] - 1s 1ms/step - loss: 1.0862 - accuracy: 0.3929\n",
            "Epoch 2/10\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 0.9620 - accuracy: 0.4759\n",
            "Epoch 3/10\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.9375 - accuracy: 0.4869\n",
            "Epoch 4/10\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.8953 - accuracy: 0.5563\n",
            "Epoch 5/10\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 0.8601 - accuracy: 0.5869\n",
            "Epoch 6/10\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.8465 - accuracy: 0.5929\n",
            "Epoch 7/10\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.6394\n",
            "Epoch 8/10\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.7691 - accuracy: 0.6967\n",
            "Epoch 9/10\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.7533 - accuracy: 0.7352\n",
            "Epoch 10/10\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.7253 - accuracy: 0.7415\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (20, 64)                  3584      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (20, 3)                   195       \n",
            "=================================================================\n",
            "Total params: 3,779\n",
            "Trainable params: 3,779\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "15/15 [==============================] - 0s 1ms/step\n",
            "accuracy:  0.6966666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64fb1a76"
      },
      "source": [
        "The accuracy of the MLP classifier is quite low considering the simplicity of this task. The major challenge of this task is that even for the same shapes, the positions where they appear in the sequence, and their sizes may vary. For example, let's look at some rectangles."
      ],
      "id": "64fb1a76"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:08.447397Z",
          "iopub.status.busy": "2021-08-05T17:09:08.446675Z",
          "iopub.status.idle": "2021-08-05T17:09:08.755589Z",
          "shell.execute_reply": "2021-08-05T17:09:08.756066Z"
        },
        "id": "aee11450"
      },
      "source": [
        "draw_curve(0)\n",
        "draw_curve(1)\n",
        "draw_curve(4)"
      ],
      "id": "aee11450",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b56f345"
      },
      "source": [
        "üë©üéì**Student question: Do you think MLPs are suitable for this task? Why or why not?**"
      ],
      "id": "6b56f345"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:08.762479Z",
          "iopub.status.busy": "2021-08-05T17:09:08.761727Z",
          "iopub.status.idle": "2021-08-05T17:09:08.763757Z",
          "shell.execute_reply": "2021-08-05T17:09:08.764232Z"
        },
        "id": "aa38ce8f"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# Not really. MLPs don't deal well with translations (moving); they are translation invariant\n",
        "# So the stretch, different shapes, change in size, and small training set all result in an\n",
        "# undesirable accuracy."
      ],
      "id": "aa38ce8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9177bcdb"
      },
      "source": [
        "### Recurrent Neural Networks (RNNs)"
      ],
      "id": "9177bcdb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76a74518"
      },
      "source": [
        "Recurrent Neural Networks (RNNs) work by iteratively applying the same base operation to each element in the input sequence. To keep track of what it has seen so far, it also maintains an internal state. We call the base operation an RNN cell. Throughout this lab, we will use a special kind of RNN cell, a Long short-term memory (LSTM) cell due to its empirical successes."
      ],
      "id": "76a74518"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a3ce605"
      },
      "source": [
        "Let's assume that we have a sequence of inputs $x_1, \\ldots, x_T$. (We're notating the input elements as if they are scalars, but you should keep in mind that they might well be vectors themselves.) Let's consider a single update operation at step $t$, where the current internal state is denoted by $h_t$:"
      ],
      "id": "9a3ce605"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba7d523c"
      },
      "source": [
        "$h_{t+1} = {\\text{LSTM}}_{\\phi} (h_t, x_t)$,"
      ],
      "id": "ba7d523c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5049ac21"
      },
      "source": [
        "where the LSTM cell has two inputs and one output: it uses both the input element $x_t$ and the old memory $h_t$ to compute the updated memory $h_{t+1}$. $\\phi$ denotes the parameters of the LSTM cell, which we can adjust during training. For simplicy, we omit these parameters througout the rest of this lab. The internal computations of the LSTM cell are beyond the scope of this course, but for anyone interested in knowing further details, [this blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) might be a good starting point."
      ],
      "id": "5049ac21"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "1ecdd0f5"
      },
      "source": [
        "Now that we have defined a single update step, we can chain them together to produce a summary of $x_1, \\ldots, x_T$, starting from $h_0=0$:"
      ],
      "id": "1ecdd0f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a89c773"
      },
      "source": [
        "\\begin{align}\n",
        "h_0 &= 0 \\\\\n",
        "h_1 &= \\text{LSTM} (h_0, x_1) \\\\ \n",
        "h_2 &= \\text{LSTM} (h_1, x_2) \\\\ \n",
        "h_3 &= \\text{LSTM} (h_2, x_3) \\\\ \n",
        "\\vdots\\\\\n",
        "h_T &= \\text{LSTM} (h_{T-1}, x_T) \\\\ \n",
        "\\end{align}\""
      ],
      "id": "6a89c773"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4f8a697"
      },
      "source": [
        "$h_T$ can be used as a feature representation for the entire input sequence $x_1, \\ldots, x_T$."
      ],
      "id": "a4f8a697"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37b12eeb"
      },
      "source": [
        "In `Keras`, the API for an LSTM cell is\n",
        "```\n",
        "LSTM(\n",
        "    units,\n",
        "    activation=\"tanh\",\n",
        "    recurrent_activation=\"sigmoid\",\n",
        "    use_bias=True,\n",
        "    kernel_initializer=\"glorot_uniform\",\n",
        "    recurrent_initializer=\"orthogonal\",\n",
        "    bias_initializer=\"zeros\",\n",
        "    unit_forget_bias=True,\n",
        "    kernel_regularizer=None,\n",
        "    recurrent_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    recurrent_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    dropout=0.0,\n",
        "    recurrent_dropout=0.0,\n",
        "    return_sequences=False,\n",
        "    return_state=False,\n",
        "    go_backwards=False,\n",
        "    stateful=False,\n",
        "    time_major=False,\n",
        "    unroll=False,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "It appears intimidating, but we only need to set `units` in this lab. In a nutshell, it controls the size of the hidden states in the LSTM cell: the larger the size, the more powerful the model will be, but the more likely the model will overfit to training data (overfitting means memorizing the training data without being able to generlize to the test data)."
      ],
      "id": "37b12eeb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:08.774585Z",
          "iopub.status.busy": "2021-08-05T17:09:08.773870Z",
          "iopub.status.idle": "2021-08-05T17:09:08.776600Z",
          "shell.execute_reply": "2021-08-05T17:09:08.775898Z"
        },
        "id": "c21ece53"
      },
      "source": [
        "from keras.layers import LSTM\n",
        "hidden_size = 32\n",
        "lstm_layer = LSTM(hidden_size)"
      ],
      "id": "c21ece53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a4117b5"
      },
      "source": [
        "This layer can be applied to a sequence of inputs $x_1, \\ldots, x_T$, and the output will be the final hidden state $h_T$. Below shows an example of how to use this layer. Note that we need to use a `Reshape` layer to add one additional dimension to the input sequence since the expected input shape of the LSTM layer is `sequence_length x input size`, and the `input_size` in the case is 1 since $x_t$'s are scalars is 1."
      ],
      "id": "9a4117b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:08.788384Z",
          "iopub.status.busy": "2021-08-05T17:09:08.787450Z",
          "iopub.status.idle": "2021-08-05T17:09:09.086880Z",
          "shell.execute_reply": "2021-08-05T17:09:09.087433Z"
        },
        "lines_to_next_cell": 2,
        "id": "f9ee4a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e61fc0d-6408-49a1-b1bb-4e443a4c4eda"
      },
      "source": [
        "input_shape = (input_length, 1)\n",
        "model = Sequential()\n",
        "model.add(Reshape(input_shape))\n",
        "model.add(lstm_layer)\n",
        "#\n",
        "# take the first example as input\n",
        "# input shape: num samples x input_length\n",
        "# output shape: num samples x hidden size\n",
        "inputs = tf.convert_to_tensor(df_train[features].iloc[:1])\n",
        "output = model(inputs)\n",
        "print (inputs.shape)\n",
        "print (output.shape)"
      ],
      "id": "f9ee4a96",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 55)\n",
            "(1, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "353e38d2"
      },
      "source": [
        "# Group Exercise A"
      ],
      "id": "353e38d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51f1cb6f"
      },
      "source": [
        "## Question 0"
      ],
      "id": "51f1cb6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1913e6a"
      },
      "source": [
        "Icebreakers"
      ],
      "id": "d1913e6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06b11fb3"
      },
      "source": [
        "Who are other members of your group today?"
      ],
      "id": "06b11fb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "06ac3cfc"
      },
      "source": [
        "üìùüìùüìùüìù Nobody else turned their cameras/microphones on in my group so I'm not sure."
      ],
      "id": "06ac3cfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8084dabb"
      },
      "source": [
        "* What's their favorite place?"
      ],
      "id": "8084dabb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "629b249d"
      },
      "source": [
        "üìùüìùüìùüìù FILLME"
      ],
      "id": "629b249d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01c0c10f"
      },
      "source": [
        "* What are their goals by the end of the decade?"
      ],
      "id": "01c0c10f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019871dc"
      },
      "source": [
        "üìùüìùüìùüìù FILLME"
      ],
      "id": "019871dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb9d91d"
      },
      "source": [
        "## Question 1"
      ],
      "id": "4cb9d91d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f73012d4"
      },
      "source": [
        "Look at this figure again. Can you figure out where are $h_t$'s and $x_t$'s? (Don't worry if you not understand the entire diagram does for now, we will elaborate on it later)**"
      ],
      "id": "f73012d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5da6e76"
      },
      "source": [
        "![image](https://www.researchgate.net/profile/Huy-Tien-Nguyen/publication/321259272/figure/fig2/AS:572716866433034@1513557749934/Illustration-of-our-LSTM-model-for-sentiment-classification-Each-word-is-transfered-to-a_W640.jpg)"
      ],
      "id": "f5da6e76"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:09.092539Z",
          "iopub.status.busy": "2021-08-05T17:09:09.091660Z",
          "iopub.status.idle": "2021-08-05T17:09:09.093827Z",
          "shell.execute_reply": "2021-08-05T17:09:09.094423Z"
        },
        "id": "bc8004b3"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# We input xt's to every LSTM cell, and we output ht's from every LSTM cell. \n",
        "# So the xt's are the words we are passing in to translate, the ht's from previous\n",
        "# LSTM cells shape the internal state, and the outputs from the LSTM cells are the ht's. "
      ],
      "id": "bc8004b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19d95683"
      },
      "source": [
        "Why can LSTMs process variable length inputs?"
      ],
      "id": "19d95683"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:09.098765Z",
          "iopub.status.busy": "2021-08-05T17:09:09.097941Z",
          "iopub.status.idle": "2021-08-05T17:09:09.099981Z",
          "shell.execute_reply": "2021-08-05T17:09:09.100678Z"
        },
        "id": "4b1f01cc"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# Since the model here is \"recurrent,\" it keeps track of the internal state at every iteration,\n",
        "# and the LSTM therefore is applying an operation dependent on the result of the last operation\n",
        "# at every step of the way, allowing variable length inputs."
      ],
      "id": "4b1f01cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3900ac9"
      },
      "source": [
        "## Question 2"
      ],
      "id": "c3900ac9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d8abbb1"
      },
      "source": [
        "Modify the MLP code to use LSTM instead. We recommend using a hidden size of 32 or 64. Train the model and report the test accuracy. You should expect to see at least 90% accuracy. Hint: don't forget the reshape layer before the LSTM!"
      ],
      "id": "6d8abbb1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:09.118319Z",
          "iopub.status.busy": "2021-08-05T17:09:09.116720Z",
          "iopub.status.idle": "2021-08-05T17:09:24.092518Z",
          "shell.execute_reply": "2021-08-05T17:09:24.092971Z"
        },
        "id": "c91bf44b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1682de7f-8986-497c-8e0b-8a3000b27e04"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "\n",
        "def create_rnn_model():\n",
        "    tf.random.set_seed(1234)\n",
        "    # create model\n",
        "    input_shape = (input_length, 1)\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape))\n",
        "    model.add(LSTM(32))\n",
        "    model.add(Dense(3, activation='softmax')) # output a vector of size 3\n",
        "    # Compile model\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                   optimizer=\"adam\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "    return model\n",
        "#\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_rnn_model,\n",
        "                         epochs=10,\n",
        "                         batch_size=20,\n",
        "                         verbose=1)\n",
        "# fit model\n",
        "model.fit(x=df_train[features], y=df_train[\"class\"])\n",
        "# print summary\n",
        "print (model.model.summary())\n",
        "# predict on test set\n",
        "df_test[\"predict\"] = model.predict(df_test[features])\n",
        "correct = (df_test[\"predict\"] == df_test[\"class\"])\n",
        "accuracy = correct.sum() / correct.size\n",
        "print (\"accuracy: \", accuracy)"
      ],
      "id": "c91bf44b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60/60 [==============================] - 4s 15ms/step - loss: 1.0924 - accuracy: 0.3909\n",
            "Epoch 2/10\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.8978 - accuracy: 0.5822\n",
            "Epoch 3/10\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.7772 - accuracy: 0.6318\n",
            "Epoch 4/10\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.6576 - accuracy: 0.7066\n",
            "Epoch 5/10\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.4706 - accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 1.0184 - accuracy: 0.5086\n",
            "Epoch 7/10\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.5751 - accuracy: 0.7174\n",
            "Epoch 8/10\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.4743 - accuracy: 0.8543\n",
            "Epoch 9/10\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.3043 - accuracy: 0.9343\n",
            "Epoch 10/10\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.1980 - accuracy: 0.9595\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_13 (Reshape)         (20, 55, 1)               0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (20, 32)                  4352      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (20, 3)                   99        \n",
            "=================================================================\n",
            "Total params: 4,451\n",
            "Trainable params: 4,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "15/15 [==============================] - 1s 6ms/step\n",
            "accuracy:  0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71a4f791"
      },
      "source": [
        "## Unit B"
      ],
      "id": "71a4f791"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56129e9e"
      },
      "source": [
        "### Recurrent Neural Networks for Text Classification"
      ],
      "id": "56129e9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80397890"
      },
      "source": [
        "Now let's go to a real application: text classification. Text classification raises new challenges, as the inputs are strings instead of the numeric values we have been familiar with in this course. In this unit, we will first find a suitable feature representation for the text input, and then we will apply an LSTM-based model to this task."
      ],
      "id": "80397890"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd13fc6d"
      },
      "source": [
        "The text classification task we will be working with is sentiment analysis,  where the goal is to classify the sentiment of a text sequence. In particular, we will use the Stanford Sentiment Treebank v2 (SST-2) dataset, where we want to predict the sentiment (positive or negative) for a movie review."
      ],
      "id": "fd13fc6d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.098484Z",
          "iopub.status.busy": "2021-08-05T17:09:24.097628Z",
          "iopub.status.idle": "2021-08-05T17:09:24.229634Z",
          "shell.execute_reply": "2021-08-05T17:09:24.230183Z"
        },
        "id": "6e771e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "54bb29d2-738a-4aac-f28f-46da7b60071a"
      },
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/srush/BT-AI/main/notebooks/sst_movie_reviews_processed_train.csv.gz', compression='gzip')\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/srush/BT-AI/main/notebooks/sst_movie_reviews_processed_test.csv.gz', compression='gzip')\n",
        "df_train[:10]"
      ],
      "id": "6e771e5a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>an</td>\n",
              "      <td>ambitious</td>\n",
              "      <td>'</td>\n",
              "      <td>what</td>\n",
              "      <td>if</td>\n",
              "      <td>?</td>\n",
              "      <td>'</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>a</td>\n",
              "      <td>great</td>\n",
              "      <td>cast</td>\n",
              "      <td>and</td>\n",
              "      <td>a</td>\n",
              "      <td>wonderful</td>\n",
              "      <td>but</td>\n",
              "      <td>sometimes</td>\n",
              "      <td>confusing</td>\n",
              "      <td>OOV</td>\n",
              "      <td>movie</td>\n",
              "      <td>about</td>\n",
              "      <td>growing</td>\n",
              "      <td>up</td>\n",
              "      <td>in</td>\n",
              "      <td>a</td>\n",
              "      <td>dysfunctional</td>\n",
              "      <td>family</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>remarkable</td>\n",
              "      <td>for</td>\n",
              "      <td>its</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>and</td>\n",
              "      <td>intensity</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>steven</td>\n",
              "      <td>soderbergh</td>\n",
              "      <td>'</td>\n",
              "      <td>s</td>\n",
              "      <td>digital</td>\n",
              "      <td>video</td>\n",
              "      <td>experiment</td>\n",
              "      <td>is</td>\n",
              "      <td>a</td>\n",
              "      <td>clever</td>\n",
              "      <td>and</td>\n",
              "      <td>cutting</td>\n",
              "      <td>,</td>\n",
              "      <td>quick</td>\n",
              "      <td>and</td>\n",
              "      <td>dirty</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "      <td>modern</td>\n",
              "      <td>living</td>\n",
              "      <td>and</td>\n",
              "      <td>movie</td>\n",
              "      <td>life</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>a</td>\n",
              "      <td>whole</td>\n",
              "      <td>lot</td>\n",
              "      <td>of</td>\n",
              "      <td>fun</td>\n",
              "      <td>and</td>\n",
              "      <td>funny</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>middle</td>\n",
              "      <td>,</td>\n",
              "      <td>though</td>\n",
              "      <td>somewhat</td>\n",
              "      <td>less</td>\n",
              "      <td>OOV</td>\n",
              "      <td>at</td>\n",
              "      <td>the</td>\n",
              "      <td>start</td>\n",
              "      <td>and</td>\n",
              "      <td>finish</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>positive</td>\n",
              "      <td>constantly</td>\n",
              "      <td>touching</td>\n",
              "      <td>,</td>\n",
              "      <td>surprisingly</td>\n",
              "      <td>funny</td>\n",
              "      <td>,</td>\n",
              "      <td>OOV</td>\n",
              "      <td>exploration</td>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>creative</td>\n",
              "      <td>act</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>negative</td>\n",
              "      <td>it</td>\n",
              "      <td>'</td>\n",
              "      <td>s</td>\n",
              "      <td>as</td>\n",
              "      <td>if</td>\n",
              "      <td>allen</td>\n",
              "      <td>,</td>\n",
              "      <td>at</td>\n",
              "      <td>OOV</td>\n",
              "      <td>,</td>\n",
              "      <td>has</td>\n",
              "      <td>OOV</td>\n",
              "      <td>challenging</td>\n",
              "      <td>himself</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>negative</td>\n",
              "      <td>check</td>\n",
              "      <td>your</td>\n",
              "      <td>brain</td>\n",
              "      <td>and</td>\n",
              "      <td>your</td>\n",
              "      <td>secret</td>\n",
              "      <td>agent</td>\n",
              "      <td>OOV</td>\n",
              "      <td>ring</td>\n",
              "      <td>at</td>\n",
              "      <td>the</td>\n",
              "      <td>door</td>\n",
              "      <td>because</td>\n",
              "      <td>you</td>\n",
              "      <td>don</td>\n",
              "      <td>'</td>\n",
              "      <td>t</td>\n",
              "      <td>want</td>\n",
              "      <td>to</td>\n",
              "      <td>think</td>\n",
              "      <td>too</td>\n",
              "      <td>much</td>\n",
              "      <td>about</td>\n",
              "      <td>what</td>\n",
              "      <td>'</td>\n",
              "      <td>s</td>\n",
              "      <td>going</td>\n",
              "      <td>on</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>negative</td>\n",
              "      <td>it</td>\n",
              "      <td>'</td>\n",
              "      <td>s</td>\n",
              "      <td>just</td>\n",
              "      <td>hard</td>\n",
              "      <td>to</td>\n",
              "      <td>believe</td>\n",
              "      <td>that</td>\n",
              "      <td>a</td>\n",
              "      <td>life</td>\n",
              "      <td>like</td>\n",
              "      <td>this</td>\n",
              "      <td>can</td>\n",
              "      <td>sound</td>\n",
              "      <td>so</td>\n",
              "      <td>dull</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>negative</td>\n",
              "      <td>the</td>\n",
              "      <td>rollerball</td>\n",
              "      <td>sequences</td>\n",
              "      <td>feel</td>\n",
              "      <td>OOV</td>\n",
              "      <td>and</td>\n",
              "      <td>OOV</td>\n",
              "      <td>.</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "      <td>PAD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class           0           1          2  ...   51   52   53   54\n",
              "0  positive          an   ambitious          '  ...  PAD  PAD  PAD  PAD\n",
              "1  positive           a       great       cast  ...  PAD  PAD  PAD  PAD\n",
              "2  positive  remarkable         for        its  ...  PAD  PAD  PAD  PAD\n",
              "3  positive      steven  soderbergh          '  ...  PAD  PAD  PAD  PAD\n",
              "4  positive           a       whole        lot  ...  PAD  PAD  PAD  PAD\n",
              "5  positive  constantly    touching          ,  ...  PAD  PAD  PAD  PAD\n",
              "6  negative          it           '          s  ...  PAD  PAD  PAD  PAD\n",
              "7  negative       check        your      brain  ...  PAD  PAD  PAD  PAD\n",
              "8  negative          it           '          s  ...  PAD  PAD  PAD  PAD\n",
              "9  negative         the  rollerball  sequences  ...  PAD  PAD  PAD  PAD\n",
              "\n",
              "[10 rows x 56 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6e58504"
      },
      "source": [
        "The column `class` stores the sentiment of each review, which is either \"positive\" or \"negative\"."
      ],
      "id": "d6e58504"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.240120Z",
          "iopub.status.busy": "2021-08-05T17:09:24.239278Z",
          "iopub.status.idle": "2021-08-05T17:09:24.242254Z",
          "shell.execute_reply": "2021-08-05T17:09:24.242759Z"
        },
        "id": "ad4589d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ea291c-60ad-4b96-ce03-43d2ccf4506a"
      },
      "source": [
        "df_train[:100][\"class\"].unique()"
      ],
      "id": "ad4589d8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf9b8233"
      },
      "source": [
        "The other columns store the words, where the i-th word is stored in a feature column i (counting from 0). For example, the first word of each movie review is stored in column 0, the second word is stored in column 1, and so on. As before, we store all feature column names in a list. The maximum length of sentences is 55 on this dataset, so we have 55 feature columns."
      ],
      "id": "bf9b8233"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.247520Z",
          "iopub.status.busy": "2021-08-05T17:09:24.246802Z",
          "iopub.status.idle": "2021-08-05T17:09:24.249087Z",
          "shell.execute_reply": "2021-08-05T17:09:24.249579Z"
        },
        "id": "9ae4a496"
      },
      "source": [
        "input_length = 55\n",
        "features = []\n",
        "for i in range(input_length):\n",
        "    features.append(str(i))"
      ],
      "id": "9ae4a496",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28526b5b"
      },
      "source": [
        "Notice that some tokens towards the end are `PAD`. They are actually placeholders to pad every sentence into the same length such that we can store them in a table."
      ],
      "id": "28526b5b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.255567Z",
          "iopub.status.busy": "2021-08-05T17:09:24.254622Z",
          "iopub.status.idle": "2021-08-05T17:09:24.262030Z",
          "shell.execute_reply": "2021-08-05T17:09:24.262817Z"
        },
        "id": "0435c4ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddfae12-8c82-417d-98ec-4fad6fe7cab7"
      },
      "source": [
        "data = df_train[features].values\n",
        "labels = df_train['class'].values\n",
        "print (labels[1], data[1])\n",
        "print (labels[8], data[8])"
      ],
      "id": "0435c4ae",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive ['a' 'great' 'cast' 'and' 'a' 'wonderful' 'but' 'sometimes' 'confusing'\n",
            " 'OOV' 'movie' 'about' 'growing' 'up' 'in' 'a' 'dysfunctional' 'family'\n",
            " '.' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD']\n",
            "negative ['it' \"'\" 's' 'just' 'hard' 'to' 'believe' 'that' 'a' 'life' 'like' 'this'\n",
            " 'can' 'sound' 'so' 'dull' '.' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD'\n",
            " 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD' 'PAD']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47daf292"
      },
      "source": [
        "### Input Representation"
      ],
      "id": "47daf292"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb7049d"
      },
      "source": [
        "Different from all examples we've seen so far, the input in text classification cannot be directly fed into a neural network, since they are strings but not numeric values. A natural idea is to associate each word type with an integer id, such that we can use those integer ids to represent words."
      ],
      "id": "4cb7049d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e14ac04"
      },
      "source": [
        "First, we need to build the mapping from word types to integer ids."
      ],
      "id": "5e14ac04"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.318576Z",
          "iopub.status.busy": "2021-08-05T17:09:24.312287Z",
          "iopub.status.idle": "2021-08-05T17:09:24.321053Z",
          "shell.execute_reply": "2021-08-05T17:09:24.321584Z"
        },
        "id": "257d3cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e706843a-4547-49e7-a6bd-fe0ff0512d5d"
      },
      "source": [
        "# build vocabulary\n",
        "word2id = {}\n",
        "id2word = {}\n",
        "unassigned_id = 0\n",
        "for review in data:\n",
        "    for token in review:\n",
        "        if token not in word2id:\n",
        "            word2id[token] = unassigned_id\n",
        "            id2word[unassigned_id] = token\n",
        "            unassigned_id += 1\n",
        "vocab_size = len(word2id)\n",
        "print ('Vocab size: ', vocab_size)"
      ],
      "id": "257d3cbc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  4823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441eaae9"
      },
      "source": [
        "With `word2id`, we can map a word to its associated id:"
      ],
      "id": "441eaae9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.326103Z",
          "iopub.status.busy": "2021-08-05T17:09:24.325400Z",
          "iopub.status.idle": "2021-08-05T17:09:24.327936Z",
          "shell.execute_reply": "2021-08-05T17:09:24.328388Z"
        },
        "id": "8772831c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914b20a0-7d30-4a41-946b-44198eb0a871"
      },
      "source": [
        "print (word2id['the'])"
      ],
      "id": "8772831c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06b6a8b9"
      },
      "source": [
        "With `id2word`, we can map an integer id to the corresponding word:"
      ],
      "id": "06b6a8b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.332980Z",
          "iopub.status.busy": "2021-08-05T17:09:24.332171Z",
          "iopub.status.idle": "2021-08-05T17:09:24.334995Z",
          "shell.execute_reply": "2021-08-05T17:09:24.335469Z"
        },
        "lines_to_next_cell": 2,
        "id": "f999f48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8492c5dc-b1e0-4818-81c6-04f1a252f28e"
      },
      "source": [
        "print (id2word[51])"
      ],
      "id": "f999f48d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "584f767a"
      },
      "source": [
        "üë©üéì**Student question: Convert the sentence \"a great cast\" to a sequence of integer ids using `word2id`.**"
      ],
      "id": "584f767a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.340693Z",
          "iopub.status.busy": "2021-08-05T17:09:24.339935Z",
          "iopub.status.idle": "2021-08-05T17:09:24.342688Z",
          "shell.execute_reply": "2021-08-05T17:09:24.343158Z"
        },
        "id": "c4a4fe57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f9016a-7cc9-4e78-9957-adf10e2b6584"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "print (word2id['a'], word2id['great'], word2id['cast'])"
      ],
      "id": "c4a4fe57",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7 8 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d28fe4d"
      },
      "source": [
        "üë©üéì**Student question: Convert a sequence of integer ids `[7, 8, 9]` to the original sentence.**"
      ],
      "id": "2d28fe4d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.350436Z",
          "iopub.status.busy": "2021-08-05T17:09:24.349653Z",
          "iopub.status.idle": "2021-08-05T17:09:24.352396Z",
          "shell.execute_reply": "2021-08-05T17:09:24.352879Z"
        },
        "lines_to_next_cell": 1,
        "id": "1eb9d21d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e97260d-a941-46c9-d289-071b3e5d5a58"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "print (id2word[7], id2word[8], id2word[9])"
      ],
      "id": "1eb9d21d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a great cast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823e016e"
      },
      "source": [
        "Now we can convert all the strings into integer ids using those mappings."
      ],
      "id": "823e016e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.358498Z",
          "iopub.status.busy": "2021-08-05T17:09:24.357788Z",
          "iopub.status.idle": "2021-08-05T17:09:24.648631Z",
          "shell.execute_reply": "2021-08-05T17:09:24.651111Z"
        },
        "id": "46b517ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "c5f34135-9120-42ad-824d-c1b425c61f9a"
      },
      "source": [
        "def word_to_id(word):\n",
        "    return word2id[word]\n",
        "df_train[features] = df_train[features].applymap(word_to_id)\n",
        "df_test[features] = df_test[features].applymap(word_to_id)\n",
        "df_train[:10]"
      ],
      "id": "46b517ca",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "      <td>18</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>27</td>\n",
              "      <td>10</td>\n",
              "      <td>28</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>29</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>34</td>\n",
              "      <td>35</td>\n",
              "      <td>7</td>\n",
              "      <td>36</td>\n",
              "      <td>10</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>39</td>\n",
              "      <td>10</td>\n",
              "      <td>40</td>\n",
              "      <td>41</td>\n",
              "      <td>42</td>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>7</td>\n",
              "      <td>46</td>\n",
              "      <td>47</td>\n",
              "      <td>48</td>\n",
              "      <td>49</td>\n",
              "      <td>10</td>\n",
              "      <td>50</td>\n",
              "      <td>20</td>\n",
              "      <td>51</td>\n",
              "      <td>52</td>\n",
              "      <td>38</td>\n",
              "      <td>53</td>\n",
              "      <td>54</td>\n",
              "      <td>55</td>\n",
              "      <td>15</td>\n",
              "      <td>42</td>\n",
              "      <td>51</td>\n",
              "      <td>56</td>\n",
              "      <td>10</td>\n",
              "      <td>57</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>positive</td>\n",
              "      <td>58</td>\n",
              "      <td>59</td>\n",
              "      <td>38</td>\n",
              "      <td>60</td>\n",
              "      <td>50</td>\n",
              "      <td>38</td>\n",
              "      <td>15</td>\n",
              "      <td>61</td>\n",
              "      <td>48</td>\n",
              "      <td>51</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>negative</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>65</td>\n",
              "      <td>4</td>\n",
              "      <td>66</td>\n",
              "      <td>38</td>\n",
              "      <td>42</td>\n",
              "      <td>15</td>\n",
              "      <td>38</td>\n",
              "      <td>67</td>\n",
              "      <td>15</td>\n",
              "      <td>68</td>\n",
              "      <td>69</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>negative</td>\n",
              "      <td>70</td>\n",
              "      <td>71</td>\n",
              "      <td>72</td>\n",
              "      <td>10</td>\n",
              "      <td>71</td>\n",
              "      <td>73</td>\n",
              "      <td>74</td>\n",
              "      <td>15</td>\n",
              "      <td>75</td>\n",
              "      <td>42</td>\n",
              "      <td>51</td>\n",
              "      <td>76</td>\n",
              "      <td>77</td>\n",
              "      <td>78</td>\n",
              "      <td>79</td>\n",
              "      <td>2</td>\n",
              "      <td>80</td>\n",
              "      <td>81</td>\n",
              "      <td>82</td>\n",
              "      <td>83</td>\n",
              "      <td>84</td>\n",
              "      <td>85</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>86</td>\n",
              "      <td>87</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>negative</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>88</td>\n",
              "      <td>89</td>\n",
              "      <td>82</td>\n",
              "      <td>90</td>\n",
              "      <td>91</td>\n",
              "      <td>7</td>\n",
              "      <td>45</td>\n",
              "      <td>92</td>\n",
              "      <td>93</td>\n",
              "      <td>94</td>\n",
              "      <td>95</td>\n",
              "      <td>96</td>\n",
              "      <td>97</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>negative</td>\n",
              "      <td>51</td>\n",
              "      <td>98</td>\n",
              "      <td>99</td>\n",
              "      <td>100</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class   0   1   2    3   4   5   6  ...  47  48  49  50  51  52  53  54\n",
              "0  positive   0   1   2    3   4   5   2  ...   6   6   6   6   6   6   6   6\n",
              "1  positive   7   8   9   10   7  11  12  ...   6   6   6   6   6   6   6   6\n",
              "2  positive  24  25  26   27  10  28  23  ...   6   6   6   6   6   6   6   6\n",
              "3  positive  29  30   2   31  32  33  34  ...   6   6   6   6   6   6   6   6\n",
              "4  positive   7  46  47   48  49  10  50  ...   6   6   6   6   6   6   6   6\n",
              "5  positive  58  59  38   60  50  38  15  ...   6   6   6   6   6   6   6   6\n",
              "6  negative  64   2  31   65   4  66  38  ...   6   6   6   6   6   6   6   6\n",
              "7  negative  70  71  72   10  71  73  74  ...   6   6   6   6   6   6   6   6\n",
              "8  negative  64   2  31   88  89  82  90  ...   6   6   6   6   6   6   6   6\n",
              "9  negative  51  98  99  100  15  10  15  ...   6   6   6   6   6   6   6   6\n",
              "\n",
              "[10 rows x 56 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c000abbf"
      },
      "source": [
        "### Word Embeddings"
      ],
      "id": "c000abbf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647caf36"
      },
      "source": [
        "Now that we can convert the original text (a sequence of strings) into a sequence of integer ids, can we directly feed that into the LSTM layer as we did for the shape classification problem?"
      ],
      "id": "647caf36"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5903bc80"
      },
      "source": [
        "If we directly use those integer ids in the neural network, we are implicitly assuming that the word with id `1001` is closer to the word with id `1002` than it is to the word with id `10`. However, the way we constructed the mappings between word types and ids does not provide this property. \n",
        "Instead of directly using those word ids, for each word id, we maintain a different vector (usually termed an embedding), which can be stored in a matrix $E$ of size `vocab_size x embedding_size`. To get the word embedding for word id i, we can simply take the i-th row in the matrix $E_i$."
      ],
      "id": "5903bc80"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4461170b"
      },
      "source": [
        "In `Keras`, this embedding matrix is maintained in an `Embedding` layer.\n",
        "```\n",
        "Embedding(\n",
        "   input_dim,\n",
        "   output_dim,\n",
        "   embeddings_initializer=\"uniform\",\n",
        "   embeddings_regularizer=None,\n",
        "   activity_regularizer=None,\n",
        "   embeddings_constraint=None,\n",
        "   mask_zero=False,\n",
        "   input_length=None,\n",
        "   **kwargs\n",
        ")\n",
        "```\n",
        "Again, we don't need to use all arguments. The only two arguments that we need to understand are: `input_dim`, which is the size of the vocabulary `vocab_size`, and `output_dim`, which is the size of the word embeddings `embedding_size`."
      ],
      "id": "4461170b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.657593Z",
          "iopub.status.busy": "2021-08-05T17:09:24.656851Z",
          "iopub.status.idle": "2021-08-05T17:09:24.679242Z",
          "shell.execute_reply": "2021-08-05T17:09:24.679768Z"
        },
        "id": "521dc4e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c379bd87-7b2f-4def-e510-0ad46f548340"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "model = Sequential()\n",
        "embedding_size = 32\n",
        "model.add(Embedding(vocab_size, embedding_size))\n",
        "# The model will take as input an integer matrix of size (num_samples, input_length).\n",
        "# The output_shape is (num_samples, input_length, embedding_size)\n",
        "#\n",
        "# take the first example as input\n",
        "inputs = tf.convert_to_tensor(df_train[features].iloc[:1])\n",
        "outputs = model(inputs)\n",
        "print (inputs.shape)\n",
        "print (outputs.shape)"
      ],
      "id": "521dc4e4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 55)\n",
            "(1, 55, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d867f83f"
      },
      "source": [
        "So now we have converted words to their word ids to their embeddings (token strings -> integer word ids -> vector word embeddings). (You might notice that the intermediate word id step is not necessary and we can directly map each word type to a word embedding: we used this intermediate word id step since tensors are easier to work with than strings, and we only need to do this conversion once for the dataset.)"
      ],
      "id": "d867f83f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae49d41d"
      },
      "source": [
        "üë©üéì**Student question: By representing words as word embeddings, are we still making implicit assumptions that the 1001-st word is closer to the 1002-nd word than it is to the 10-th word?**"
      ],
      "id": "ae49d41d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.683941Z",
          "iopub.status.busy": "2021-08-05T17:09:24.683194Z",
          "iopub.status.idle": "2021-08-05T17:09:24.685463Z",
          "shell.execute_reply": "2021-08-05T17:09:24.686082Z"
        },
        "id": "08ce0cf1"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# Because the numbers are assigned randomly, we are not making any assumptions\n",
        "# about relations between word order. The 1001th word and the 1002nd word \n",
        "# are not necessarily closer to each other. We use the embeddings to train the \n",
        "# neural network to start recognizing patterns between words, not their initial order."
      ],
      "id": "08ce0cf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c40e0579"
      },
      "source": [
        "### Putting Everything Together"
      ],
      "id": "c40e0579"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7e8b9d1"
      },
      "source": [
        "Now we can put everything together and assemble a model for text classification: we have converted the token strings into word ids. The model first uses an embedding layer to convert those word ids into word embeddings, then the LSTM runs on top of those word embeddings, and we use a final projection layer to project to the output shape."
      ],
      "id": "e7e8b9d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e147df"
      },
      "source": [
        "# Group Exercise B"
      ],
      "id": "15e147df"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227141f0"
      },
      "source": [
        "## Question 1"
      ],
      "id": "227141f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ea9888"
      },
      "source": [
        "Take another look at this model diagram. Can you explain what's happening in this diagram? What are the modules used? What are the inputs and outputs of each module?"
      ],
      "id": "a1ea9888"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2181ada"
      },
      "source": [
        "![image](https://www.researchgate.net/profile/Huy-Tien-Nguyen/publication/321259272/figure/fig2/AS:572716866433034@1513557749934/Illustration-of-our-LSTM-model-for-sentiment-classification-Each-word-is-transfered-to-a_W640.jpg)"
      ],
      "id": "b2181ada"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.692272Z",
          "iopub.status.busy": "2021-08-05T17:09:24.691451Z",
          "iopub.status.idle": "2021-08-05T17:09:24.693607Z",
          "shell.execute_reply": "2021-08-05T17:09:24.694108Z"
        },
        "lines_to_next_cell": 1,
        "id": "fd180be5"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# To begin the process we enginner our input features. We take our token strings, \n",
        "# convert them to word ids, then convert these to vector embeddings using the Keras Embeddings layer. \n",
        "# We then pass these embeddings into the LSTM cells, using LSTM from Keras and the model infrastructure from\n",
        "# Keras and Sci-Kit Learn that allows us to construct a model with an LSTM layer. \n",
        "# We maintain state from previous LSTM cells to generate a final ht, that is then passed into the neural network, \n",
        "# which is created like previous models using Keras and Sci-Kit Learn and \n",
        "# is trained to find connections betwwen similar word embeddings to determine the final sentiment using a projection\n",
        "# layer."
      ],
      "id": "fd180be5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c31df62e"
      },
      "source": [
        "## Question 2"
      ],
      "id": "c31df62e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2963af20"
      },
      "source": [
        "Finish the `TODO`s in the `create_rnn_model` function, train the network and report the test accuracy."
      ],
      "id": "2963af20"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:24.720074Z",
          "iopub.status.busy": "2021-08-05T17:09:24.719309Z",
          "iopub.status.idle": "2021-08-05T17:09:35.891845Z",
          "shell.execute_reply": "2021-08-05T17:09:35.892330Z"
        },
        "id": "dbdf4761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727bac8d-d434-4571-f2c3-d50d37b1575d"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME    \n",
        "def create_rnn_model():\n",
        "    tf.random.set_seed(1234)\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    # TODO: add embedding layer\n",
        "    model.add(Embedding(vocab_size, 32)) # output size: length, 32\n",
        "    # TODO: add LSTM layer\n",
        "    model.add(LSTM(32))\n",
        "    model.add(Dense(2, activation='softmax')) \n",
        "    # Compile model\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "#\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_rnn_model,\n",
        "                        epochs=6,\n",
        "                        batch_size=150,\n",
        "                        verbose=1)\n",
        "# fit model\n",
        "model.fit(x=df_train[features], y=df_train[\"class\"])\n",
        "# print summary\n",
        "print (model.model.summary())\n",
        "# predict on test set\n",
        "df_test[\"predict\"] = model.predict(df_test[features])\n",
        "correct = (df_test[\"predict\"] == df_test[\"class\"])\n",
        "accuracy = correct.sum() / correct.size\n",
        "print (\"accuracy: \", accuracy)"
      ],
      "id": "dbdf4761",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "47/47 [==============================] - 5s 43ms/step - loss: 0.6928 - accuracy: 0.5062\n",
            "Epoch 2/6\n",
            "47/47 [==============================] - 2s 42ms/step - loss: 0.6925 - accuracy: 0.5193\n",
            "Epoch 3/6\n",
            "47/47 [==============================] - 2s 42ms/step - loss: 0.6804 - accuracy: 0.5501\n",
            "Epoch 4/6\n",
            "47/47 [==============================] - 2s 43ms/step - loss: 0.5050 - accuracy: 0.7672\n",
            "Epoch 5/6\n",
            "47/47 [==============================] - 2s 43ms/step - loss: 0.3202 - accuracy: 0.8787\n",
            "Epoch 6/6\n",
            "47/47 [==============================] - 2s 42ms/step - loss: 0.2308 - accuracy: 0.9160\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 32)          154336    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 162,722\n",
            "Trainable params: 162,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "13/13 [==============================] - 1s 14ms/step\n",
            "accuracy:  0.8034047226798462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a849098b"
      },
      "source": [
        "## Question 3"
      ],
      "id": "a849098b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d492dbf8"
      },
      "source": [
        "Word embeddings might sound like a very abstract concept: we are associating each word with a vector, but what do these vectors mean? What properties do they possess? In this question, we will use the [Tensorflow Embedding Projector](https://projector.tensorflow.org/) to explore some pretrained word embeddings. (We can also take our trained model and visualize the embeddings from the embedding layer, but we usually need to train on very large datasets to see meaningful visualizations)"
      ],
      "id": "d492dbf8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cf88626"
      },
      "source": [
        "[Embedding Projector](https://projector.tensorflow.org/)"
      ],
      "id": "0cf88626"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4923fa4e"
      },
      "source": [
        "This visualization tool visualizes word embeddings in a 3-D space, but keep in mind that those embeddings are actually of much higher dimensionality (`embedding_size` is 200 in the default setting), and their neighbors are found in the original (200-D) space, not the 3-D space, which might lead to some seemingly nearby points not being shown as nearest neighbors."
      ],
      "id": "4923fa4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea316b23"
      },
      "source": [
        "* Use the search tool in the right-side panel and search \"smith\". The point cloud in the middle pannel will show this word as well as its nearest neighbors. What did you observe about the neighbors of the word \"smith\"?"
      ],
      "id": "ea316b23"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:35.898192Z",
          "iopub.status.busy": "2021-08-05T17:09:35.897004Z",
          "iopub.status.idle": "2021-08-05T17:09:35.901038Z",
          "shell.execute_reply": "2021-08-05T17:09:35.900440Z"
        },
        "id": "b09d07ca"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# A bunch of other names come up as nearest neighbors. Some are first names like Jospeh and William, \n",
        "# while others are more likely to be surnames, like Harris and Murray. Additionally, most of the names\n",
        "# seem to be names traditionally given to or associated with men. "
      ],
      "id": "b09d07ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc716b2d"
      },
      "source": [
        "* Let's try another word \"apple\" and find its nearest neighbors. What's your observation? Is \"apple\" considered a fruit or a company here? Do you consider this an issue for using word embeddings to represent text?"
      ],
      "id": "bc716b2d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:35.906719Z",
          "iopub.status.busy": "2021-08-05T17:09:35.905299Z",
          "iopub.status.idle": "2021-08-05T17:09:35.908623Z",
          "shell.execute_reply": "2021-08-05T17:09:35.909492Z"
        },
        "id": "36ef93c0"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# A lot of the words that come up are technology related like Macintosh, Microsoft, OS, IBM, etc.\n",
        "# Apple is onsidered a company here, which points to the issue of having one word that holds mutliple meanings.\n",
        "# This seems like it would make word embedding more difficult, since the meaning behind a word is context-specific\n",
        "# and changeable. "
      ],
      "id": "36ef93c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2adf111"
      },
      "source": [
        "## Question 4"
      ],
      "id": "f2adf111"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56a23611"
      },
      "source": [
        "Word embeddings are numeric representations of word types, hence they support algebraic operations. For example, we cannot compute `water + bird - air` in the string space, but we can compute `embedding_of_water + embedding_of_bird - embedding_of_air`. Then we can convert the resulting vector back to word by finding its nearest neighbors like we did in the previous question."
      ],
      "id": "56a23611"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddd089a"
      },
      "source": [
        "Let's use this demo to perform word algebra."
      ],
      "id": "0ddd089a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a865f40"
      },
      "source": [
        "[Word Algebra](https://turbomaze.github.io/word2vecjson/)"
      ],
      "id": "4a865f40"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74573262"
      },
      "source": [
        "* Use the tool under section \"Word Algebra\" to find the nearest neighbors of `water + bird -air`. What do you get?"
      ],
      "id": "74573262"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:35.914254Z",
          "iopub.status.busy": "2021-08-05T17:09:35.913417Z",
          "iopub.status.idle": "2021-08-05T17:09:35.915986Z",
          "shell.execute_reply": "2021-08-05T17:09:35.916539Z"
        },
        "id": "e5649fa1"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# I get: bird, birds, water, fish, turtle, frog, frogs, owl, animals, snake"
      ],
      "id": "e5649fa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "453a0189"
      },
      "source": [
        "* Can you find some other interesting examples?"
      ],
      "id": "453a0189"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-05T17:09:35.923783Z",
          "iopub.status.busy": "2021-08-05T17:09:35.922890Z",
          "iopub.status.idle": "2021-08-05T17:09:35.925211Z",
          "shell.execute_reply": "2021-08-05T17:09:35.925770Z"
        },
        "id": "f7fd302a"
      },
      "source": [
        "#üìùüìùüìùüìù FILLME\n",
        "# I did \"teacher\" + \"apple\" - \"doctor\" to see whether the correct association was made for apple.\n",
        "# I got: apple, teacher, doctor, teachers, elementary, math, classroom, intel, teaching, kindergarten\n",
        "\n",
        "# I also did \"leaves\" + \"tree\" - \"house\" to see how leaves was interpreted.\n",
        "# I got: leaves, tree, trees, leaf, house, flower, arrives, leaving, flowers, fruit"
      ],
      "id": "f7fd302a",
      "execution_count": null,
      "outputs": []
    }
  ]
}